---
title: "Data Science I, Workshop I: Predicting interest rates at the Lending Club"
author: "12"
date: "30/09/2022"
output:
  html_document:
    theme: journal
    highlight: tango
    number_sections: yes
    toc: yes
    toc_float: no
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, load_libraries, include = FALSE}
library(tidyverse) # the usual stuff: dplyr, readr, and other goodies
library(lubridate) # to handle dates
library(GGally) # for correlation-scatterplot matrix
library(car) # vif() function to check for multicolinearity
library(ggfortify) # to produce residual diagnostic plots
library(rsample) # to split dataframe in training- & testing sets
library(here) # to read files and organise data
library(janitor) # clean_names()
library(broom) # use broom:augment() to get tidy table with regression output, residuals, etc
library(huxtable) # to get summary table of all models produced
library(caret) # to train more advanced models (k-fold cross-validation, stepwise regression, LASSO)
library(zoo) #to allow for timeseries operations
library(formatR)
```

This workshop partially replicates the analysis presented in class (lectures 1 and 2) and builds on it. You are to work in your study group. Feel free to refer to the `Lending_Club_Session1_and_2.html` if you get stuck.

The workshop consists of 13 questions. Submit one report (one file) per study group. 

*Please write your answer below each question and submit a knitted RMD markup file as the HTML file via Canvas, within 6 days after the end of this workshop. Please note that there is a 20% penalty on your Workshop Report 1 grade if you submit an RMD file.*

Please keep your answers concise -- focus on answering what you are asked and use your data science work to justify your answers. Do not focus on the process you have followed to reach the answer. Note that some chunks in the RMD code specifies `eval = F` to prevent errors as this file is incomplete. Make sure to remove this option from the completed chunks.

# Load and prepare the data

We start by loading the data to R in a dataframe.
```{r, load_data, warning=FALSE, message=FALSE}

lc_raw <- read_csv("LendingClub Data.csv",  skip=1) %>%  #since the first row is a title we want to skip it. 
  clean_names() # use janitor::clean_names()
```

# ICE the data: Inspect, Clean, Explore

Any data science engagement starts with ICE. Inspect, Clean and Explore the data. For this workshop we have cleaned the data for you. 


```{r}
glimpse(lc_raw[, 1:10]) 


lc_clean<- lc_raw %>%
  dplyr::select(-x20:-x80) %>% #delete empty columns
  filter(!is.na(int_rate)) %>%   #delete empty rows
  mutate(
    issue_d = mdy(issue_d),  # lubridate::mdy() to fix date format
    term = factor(term_months),     # turn 'term' into a categorical variable
    delinq_2yrs = factor(delinq_2yrs) # turn 'delinq_2yrs' into a categorical variable
  ) %>% 
  dplyr::select(-emp_title,-installment, -term_months, everything()) #move some not-so-important variables to the end. 


glimpse(lc_clean[, 1:10]) 
```

The data is now in a clean format stored in the dataframe "lc_clean." 

# **Q1.** Explore the data by building some visualizations as suggested below. Please add at least *two* visualizations of your own.{-}

Provide your answers in the code block below. (Look at the "Lending_Club_Session1_and_2.html" for some hints on how to do this using ggplot.)

```{r, data_visualisation, eval = F}
# Build a histogram of interest rates. Make sure it looks nice!


# Build a histogram of interest rates but use different color for loans of different grades 
  lc_clean %>%  ggplot( aes(int_rate, fill = grade)) + 
    geom_histogram(bins = 20) + 
    theme_bw() +
    theme(legend.position = "right") +
    labs(title = "Lower risk grades result in lower interest rates charged.",
         subtitle ="Histogram showing the distribution of interest rates segmented per risk grade.",
         caption ="Source: Lending Club (2022)",
         x = "Risk Grade",
         y = "Interest Rate")

# Produce a scatter plot of loan amount against interest rate and add visually the line of best fit
scatterplot_loan_interest <- ggplot(lc_clean, aes(x = loan_amnt, y = int_rate) ) +
  geom_point() +
  geom_smooth() +
  labs(
    title = '10k+ loan amounts is associated with higher interest rate',
    subtitle = 'Relationship between loan amount and interest rate',
    x = 'Loan Amount',
    y = 'Interest Rate',
    caption="Source: Lending Club (2022)"
  ) +
  theme_bw()

scatterplot_loan_interest


# Produce a scatter plot of annual income against interest rate and add visually the line of best fit 


# In the same axes, produce box plots of the interest rate for every value of delinquencies


# Add 2 visualizations of your own
# boxplot of interest rate based on home ownership
lc_clean %>%
  ggplot(aes(x= int_rate, y = home_ownership))+
  geom_boxplot() 

```

# Estimate simple linear regression models

We start with a simple but quite powerful model. Use the `lm` command to estimate a regression model with the following variables "term",  "annual_inc", "dti", and "grade".

```{r, simple regression, eval = F}

model0 <- lm(int_rate ~ term + annual_inc + dti + grade, data = lc_clean)
summary(model0)

```

# **Q2.** Answer the following questions on model 0. {-}
a. Are all variables statistically significant?

> All variables are significant. 

b. How much explanatory power does the model have? 

>  Around 91.91% of variance in the interest rate is explained by our current model

Fit a new linear regression that is identical to model 0, however, with the extra variable "loan_amnt".

```{r, simple regression with extra variable, eval = F}

model1 <-lm( int_rate ~ term + annual_inc + dti + grade + loan_amnt, data = lc_clean
  )
summary(model1)

```

# **Q3.** Answer the following questions on model 1. {-}

a. Are all variables statistically significant?

> No, the annual income variable is not significant as the p-value is over 0.05. All others are below 0.05.

b. If your answer to part 'a' is affirmative, then discuss whether model 0 should be preferred over model 1. Otherwise, discuss how a variable that was statistically significant in model 0 is not significant anymore; or how the new variable is not statistically significant.

> Loan amount and annual income are related, 

c. How do you interpret the coefficients of the *(i)* "term60" dummy variable; *(ii)* "gradeF" dummy variable; and *(iii)* "loan_amnt" variable?
d. How much explanatory power does the model have? 
e. Approximately, how wide would the 95% confidence interval of any prediction based on this model be? 

>Answer here:

# Feature Engineering

Let's build progressively more complex models, with more features.

```{r, Feature Engineering, eval = F}
#Add to model 1 an interaction between loan amount and grade. Use the "var1*var2" notation to define an interaction term in the linear regression model. This will add the interaction and the individual variables to the model. 

model2 <- lm( int_rate ~ term + annual_inc + dti + grade*loan_amnt, data = lc_clean)
summary(model2)
#Add to the model you just created above the square and the cube of annual income. Use the poly(var_name,3) command as a variable in the linear regression model.  

model3 <-lm( int_rate ~ poly(annual_inc, 3)  + term  + dti + grade*loan_amnt , data = lc_clean)
summary(model3)
#Continuing with the previous model, instead of annual income as a continuous variable break it down into quartiles and use quartile dummy variables. This is an alternative way of modelling non-linear relationships. You can do this with the following command. 
  
lc_clean <- lc_clean %>% 
  mutate(quartiles_annual_inc = as.factor(ntile(annual_inc, 4)))

model4 <- lm( int_rate ~ quartiles_annual_inc  + term  + dti + grade*loan_amnt , data = lc_clean)
summary(model4)  

#Compare the performance of these four models using the `anova` command
anova(model1, model2, model3, model4)
  
```

# **Q4.** Answer the following questions {-}
a. Which of the four models has the most explanatory power in sample? Without testing on a test set, which model would you say has the highest out of sample explanatory power? 

b. In model 1, how do you interpret the estimated coefficient of the interaction term between grade C and loan amount?\
c. The problem of multicollinearity describes the situations where one feature is highly correlated with other features (or with a linear combination of other features). If your goal is to use the model to make predictions, should you be concerned by the problem of multicollinearity? Why, or why not?

>Answer here:


# Out of sample testing
Let's check the predictive accuracy of model2 by holding out a subset of the data to use as a testing data set. This method is sometimes referred to as the hold-out method for out-of-sample testing. The code below is incomplete -- complete in light of the comments.

```{r, out of sample testing, eval = F}
# split the data in dataframe called "testing" and another one called  "training". The "training" dataframe should have 80% of the data and the "testing" dataframe 20%.
set.seed(23)
train_test_split <- initial_split(lc_clean, prop = 0.80
                                  )
training <- training(train_test_split)
testing <- testing(train_test_split)

#Fit model2 on the training set 
model2_training <- lm( int_rate ~ term + annual_inc + dti + grade*loan_amnt, data = training)
summary(model2_training)

# Calculate the RMSE of the model in the training set (in sample)
rmse_training <- residuals(model2_training)^2 %>% 
              mean() %>% 
              sqrt()

# Use the model to make predictions out of sample in the testing set
pred<-predict(model2_training,testing)
# Calculate the RMSE of the model in the testing set (out of sample)
rmse_testing<- RMSE(pred,testing$int_rate)
rmse_testing
```

# **Q5.** How much does the predictive accuracy of Model 2 deteriorate when we move from in-sample to out-of-sample testing? Is this sensitive to the random seed chosen? Is there any evidence of overfitting? {-}

>Answer here:

# k-fold cross validation

We can also do out of sample testing using the method of k-fold cross validation which has several advantages over the traditional hold-out method. Using the `caret` package this is easy.

{r, k-fold cross validation, eval = F}
#the method "cv" stands for cross validation. We re going to create 10 folds.  

control <- trainControl (
    method="cv",
    number=5,
    verboseIter=TRUE) #by setting this to true the model will report its progress after each estimation

#we are going to train the model and report the results using k-fold cross validation

plsFit<-train(
    int_rate ~ loan_amnt + term+ dti + annual_inc + grade +grade:loan_amnt ,
    lc_clean,
   method = "lm",
    trControl = control
   )
  

summary(plsFit)



# 15-fold cross validation

We can also do out of sample testing using the method of k-fold cross validation which has several advantages over the traditional hold-out method. Using the `caret` package this is easy.

```{r, k-fold cross validation, eval = F}
#the method "cv" stands for cross validation. We re going to create 10 folds.  

control <- trainControl (
    method="cv",
    number=15,
    verboseIter=TRUE) #by setting this to true the model will report its progress after each estimation

#we are going to train the model and report the results using k-fold cross validation

plsFit<-train(
    int_rate ~ loan_amnt + term+ dti + annual_inc + grade +grade:loan_amnt ,
    lc_clean,
   method = "lm",
    trControl = control
   )
  

summary(plsFit)
```

# **Q6.** Compare the out-of-sample RMSE of 10-fold cross validation and the hold-out method. Are they different? Which do you think is more reliable? Are there any drawbacks to the k-fold cross validation method compared to the hold-out method? Determine the out-of-sample RMSE based on 5-fold or 15-fold cross validation.  What you can infer from the outputs about the robusness of your model? {-} 

>Answer here:

# Sample size estimation and learning curves

We can use the hold out method for out-of-sample testing to check if we have a sufficiently large sample to estimate the model reliably. The idea is to set aside some of the data as a testing set. From the remaining data draw progressively larger training sets and check how the performance of the model on the testing set changes. If the performance no longer improves with larger training sets we know we have a large enough sample.  The code below does this. Examine it and run it with different random seeds. 

```{r, learning curves, eval = F}
# select a testing dataset (25% of all data)
set.seed(12)

train_test_split <- initial_split(lc_clean, prop = 0.75)
remaining <- training(train_test_split)
testing <- testing(train_test_split)

# We are now going to run 30 models starting from a tiny training set drawn from the training data and progressively increasing its size. The testing set remains the same in all iterations.

#initiating the model by setting some parameters to zero
rmse_sample<-0
sample_size<-0
Rsq_sample<-0

for(i in 1:30) {
#from the remaining dataset select a smaller subset to training the data
set.seed(100)
sample

  learning_split <- initial_split(remaining, prop = i/100)
  training <- training(learning_split)
  sample_size[i]=nrow(training)
  
  # training the model on the small dataset
  model3<-lm(int_rate ~ loan_amnt + term+ dti + annual_inc + grade + grade:loan_amnt, training)
  # test the performance of the model on the large testing dataset. This stays fixed for all iterations.
  pred<-predict(model3,testing)
  rmse_sample[i]<-RMSE(pred,testing$int_rate)
  Rsq_sample[i]<-R2(pred,testing$int_rate)
}
plot(sample_size,rmse_sample)
plot(sample_size,Rsq_sample)
```

# **Q7.** Using the learning curves above, approximately how large of a sample size would we need to estimate model 3 reliably? Once we reach this sample size, if we want to reduce the prediction error further what options do we have?{-}

>Answer here:


# Regularization using LASSO regression

If we are in the region of the learning curve where we do not have enough data, one option is to use a regularization method such as LASSO.

Let's try to estimate a large and complicated model (many interactions and polynomials) on a small training dataset using OLS regression and hold-out validation method.

```{r, OLS model overfitting, eval = F}

#split the data in testing and training. The training test is really small.

set.seed(42)
train_test_split <- initial_split(lc_clean, prop = 0.01)
training <- training(train_test_split)
testing <- testing(train_test_split)

model_lm<-lm(int_rate ~ poly(loan_amnt,3) + term + dti + annual_inc + grade + grade:poly(loan_amnt,3):term + poly(loan_amnt,3):term + grade:term, training)
predictions <- predict(model_lm,testing)

# Model prediction performance

data.frame(
  RMSE = RMSE(predictions, testing$int_rate),
  Rsquare = R2(predictions, testing$int_rate)
)
```

Not surprisingly this model does not perform well -- as we knew form the learning curves we constructed for a simpler model we need a lot more data to estimate this model reliably. Try running it again with different seeds. The model's performance tends to be sensitive to the choice of the training set.

LASSO regression offers one solution -- it extends the OLS regression by penalizing the model for setting any coefficient estimate to a value that is different from zero. The penalty is proportional to a parameter $\lambda$ (pronounced lambda). This parameter cannot be estimated directly (and for this reason sometimes it is referred to as hyperparameter). $\lambda$  will be selected through k-fold cross validation so as to provide the best out-of-sample performance.  As a result of the LASSO procedure, only those features that are more strongly associated with the outcome will have non-zero coefficient estimates and the estimated model will be less sensitive to the training set. Sometimes LASSO regression is referred to as regularization. 
1
```{r, LASSO compared to OLS, warning=FALSE, message=FALSE, eval = F}
# we will look for the optimal lambda in this sequence (we will try 1000 different lambdas, feel free to try more if necessary)
lambda_seq <- seq(0, 0.01, length = 1000)

# lasso regression using k-fold cross validation to select the best lambda

lasso <- train(
 int_rate ~ poly(loan_amnt,3) + term+ dti + annual_inc + grade +grade:poly(loan_amnt,3):term + poly(loan_amnt,3):term + grade:term,
 data = training,
 method = "glmnet",
  preProc = c("center", "scale"), #This option standardizes the data before running the LASSO regression
  trControl = control,
  tuneGrid = expand.grid(alpha = 1, lambda = lambda_seq) #alpha=1 specifies to run a LASSO regression.
  )

# Model coefficients
coef(lasso$finalModel, lasso$bestTune$lambda)

# Best lambda
lasso$bestTune$lambda

# Count of how many coefficients are greater than zero and how many are equal to zero

sum(coef(lasso$finalModel, lasso$bestTune$lambda)!=0)
sum(coef(lasso$finalModel, lasso$bestTune$lambda)==0)

# Make predictions
predictions <- predict(lasso,testing)

# Model prediction performance

data.frame(
  RMSE = RMSE(predictions, testing$int_rate),
  Rsquare = R2(predictions, testing$int_rate)
)

```

# **Q8.** Answer the following questions {-}
a. Which model performs best out of sample, OLS regression or LASSO? Why?
b. What value of lambda offers best performance? Is this sensitive to the random seed? Why?
c. How many coefficients are zero and how many are non-zero in the LASSO model of best fit? Is number of zero (or non-zero) coefficients sensitive on the random seed? Why?
d. Why is it important to standardize continuous variables before running LASSO? 

>Answer here: 

Recall that LASSO regression is almost identical to the OLS, however, there is an extra penalty term that sums the absolute values of the regression coefficients and multiplies this value with some positive penalty parameter. Formally, LASSO uses an $\ell_1$-norm, which is defined as $\ell_1(x) = \sum_{i=1}^n |x_i|$ for $x \in \mathbb{R}^n$. Another popular regularization method is the Ridge regression which penalizes the sum of squares of the coefficients. Formally, the $\ell_2$-norm is used, which is defined as $\ell_2(x) = \sqrt{\sum_{i=1}^n x_i^2}$; however, for the ease of derivations, we typically penalize the sum of squares $\ell_2^2(x) = \sum_{i=1}^n x_i^2$ without loss of generality. Run a Ridge regression and answer the following questions. (Hint. using `alpha=1` in `tuneGrid` called LASSO regression. Find how you can adjust this to call Ridge regression.)

# **Q9.** Answer the following questions {-}
a. Which model performs best out of sample, OLS regression, LASSO, or Ridge? 
b. What value of lambda offers best performance?
c. How many coefficients are zero and how many are non-zero in the Ridge model of best fit? How does it compare with the LASSO regression and what is the reason of the difference?
d. Is it important to standardize continuous variables before running Ridge as in LASSO? Or is it less important for the Ridge regression?
e. In class, we studied the advantages of LASSO regression over the standard OLS. Discuss one potential advantage of using the Ridge regression over the OLS.

>Answer here: 

# Using Time Information

Let's try to further improve the model's predictive performance. So far we have not used any time series information. Effectively, all things being equal, our prediction for the interest rate of a loan given in 2009 would be the same as that of a loan given in 2011. Is this a good assumption?
 
First, investigate graphically whether there are any time trends in the interest rates. (Note that the variable "issue_d" only has information on the month the loan was awarded but not the exact date.) Can you use this information to further improve the forecasting accuracy of your model? Try controlling for time in a linear fashion (i.e., a linear time trend) and controlling for time as quarter-year dummies (this is a method to capture non-linear effects of time -- we assume that the impact of time doesn't change within a quarter but it can chance from quarter to quarter). Finally, check if time affect loans of different grades differently.

```{r, time trends, eval = F}

#linear time trend -- concentrate on the interest rate as a function of the time (add code below)
lc_timetrend <- lc_clean %>%
  mutate(timetrend = quarter(issue_d)) %>%
  
  ggplot(aes(x = timetred, y = int_rate))

#linear time trend by grade -- similar plot, however, group by grades (via different colors) (add code below)

#Train models using OLS regression and k-fold cross-validation
#The first model has some explanatory variables and a linear time trend

time1<-train(
  int_rate ~ ,#fill your variables here "+ issue_d"
  lc_clean,
  method = "lm",
  trControl = control)

summary(time1)

#The second model has a different linear time trend for each grade class (hint. use interactions)
time2<-train(
    int_rate ~ , #fill your variables here 
    lc_clean,
   method = "lm",
    trControl = control
   )
summary(time2)

#Change the time trend to a quarter dummy variables.
#zoo::as.yearqrt() creates quarter dummies 
lc_clean_quarter<-lc_clean %>%
  mutate(yq = as.factor(as.yearqtr(lc_clean$issue_d, format = "%Y-%m-%d")))

time3<-train(
    int_rate ~ ,#fill your variables here 
    lc_clean_quarter,
     method = "lm",
    trControl = control
   )
summary(time3)

#We specify one quarter dummy variable for each grade. This is going to be a large model as there are 19 quarters x 7 grades = 133 quarter-grade dummies.
time4<-train(
    int_rate ~  ,#fill your variables here 
    lc_clean_quarter,
     method = "lm",
    trControl = control
   )
summary(time4)

data.frame(
  time1$results$RMSE,
  time2$results$RMSE,
  time3$results$RMSE,
  time4$results$RMSE)
```

# **Q10.** Based on your analysis above, is there any evidence to suggest that interest rates change over time? Does including time trends / quarter-year dummies improve predictions? Any improvement in prediction performance? {-}

>Answer here:

# Using Bond Yields 
One concern with using time trends for forecasting is that in order to make predictions for future loans we will need to project trends to the future. This is an extrapolation that may not be reasonable, especially if macroeconomic conditions in the future change. Furthermore, if we are using quarter-year dummies, it is not even possible to estimate the coefficient of these dummy variables for future quarters.

Instead, perhaps it is better to find the reasons as to why different periods are different from one another. The csv file "MonthBondYields.csv" contains information on the yield of US Treasuries on the first day of each month. Can you use it to see if you can improve your predictions without using time dummies? 


```{r, bond yields, eval = F}
#load the data to memory as a dataframe
bond_prices<-readr::read_csv("MonthBondYields.csv")

#make the date of the bond file comparable to the lending club dataset
#for some regional date/number (locale) settings this may not work. If it does try running the following line of code in the Console
#Sys.setlocale("LC_TIME","English")
bond_prices <- bond_prices %>%
  mutate(Date2=as.Date(paste("01",Date,sep="-"),"%d-%b-%y")) %>%
  select(-starts_with("X"))

#let's see what happened to bond yields over time. Lower bond yields mean the cost of borrowing has gone down.

bond_prices %>%
  ggplot(aes(x=Date2, y=Price))+geom_point(size=0.1, alpha=0.5)

#join the data using a left join
lc_with_bonds<-lc_clean %>%
  left_join(bond_prices, by = c("issue_d" = "Date2")) %>%
  arrange(issue_d) %>%
  filter(!is.na(Price)) #drop any observations where there re no bond prices available

# investigate graphically if there is a relationship 
lc_with_bonds%>%
  ggplot(aes(x=int_rate, y=Price))+geom_point(size=0.1, alpha=0.5)+geom_smooth(method="lm")

lc_with_bonds%>%
  ggplot(aes(x=int_rate, y=Price, color=grade))+geom_point(size=0.1, alpha=0.5)+geom_smooth(method="lm")

#let's train a model using the bond information

plsFit<-train(
    int_rate ~  , #fill your variables here -- try adding Price and grade.
    lc_with_bonds,
   method = "lm",
    trControl = #add 10-fold CV control here
   )
summary(plsFit)
```
# **Q11.** Do bond yields have any explanatory power? Do not forget to interpret coefficients. {-}

>Answer here: 

# **Q12.** Choose a model and describe your methodology {-}
Feel free to investigate more models with different features using the methodologies covered so far. Present the model you believe predicts interest rates the best. Describe how good it is (including the length of the 95% Confidence Interval of predictions that use this model) and what features it uses. What methodology did you use to choose it? (Do not use time trends or quarter-year dummies in your model as the first cannot be extrapolated into the future reliably and the second cannot be even estimated for future quarters.)

>Answer here:

# **Q13.** (optional) Use other publicly available datasets to further improve performance (e.g., quarterly data on US inflation, [CPI](https://fred.stlouisfed.org/series/CPALTT01USQ657N), [unemployment rate](https://fred.stlouisfed.org/series/UNRATE)). Explain why you think the additional data will make a difference and check if it does. {-}

