---
title: "Data Science I, Workshop I: Predicting interest rates at the Lending Club"
author: "12"
date: "10/10/2022"
output:
  html_document:
    theme: journal
    highlight: tango
    number_sections: yes
    toc: yes
    toc_float: no
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, load_libraries, include = FALSE}
library(tidyverse) # the usual stuff: dplyr, readr, and other goodies
library(lubridate) # to handle dates
library(GGally) # for correlation-scatterplot matrix
library(car) # vif() function to check for multicolinearity
library(ggfortify) # to produce residual diagnostic plots
library(rsample) # to split dataframe in training- & testing sets
library(here) # to read files and organise data
library(janitor) # clean_names()
library(broom) # use broom:augment() to get tidy table with regression output, residuals, etc
library(huxtable) # to get summary table of all models produced
library(caret) # to train more advanced models (k-fold cross-validation, stepwise regression, LASSO)
library(zoo) #to allow for timeseries operations
library(formatR)
```


# Determining interest rates

The Lending Club provides loans to various parties. To aid the Lending Club in their operations, we are developing an algorithm that will predict the interest rate to be assigned to the loan. We take a dataset with previous lenders from 2007 and the end of 2011. 


```{r, load_data, warning=FALSE, message=FALSE}

lc_raw <- read_csv("LendingClub Data.csv",  skip=1) %>%  #since the first row is a title we want to skip it. 
  clean_names() # use janitor::clean_names()
```

## ICE the data: Inspect, Clean, Explore

Any data science engagement starts with ICE. Inspect, Clean and Explore the data. For this workshop we have cleaned the data for you. 


```{r}
library(skimr)
glimpse(lc_raw[, 1:10]) 


lc_clean<- lc_raw %>%
  dplyr::select(-x20:-x80) %>% #delete empty columns
  filter(!is.na(int_rate)) %>%   #delete empty rows
  mutate(
    issue_d = mdy(issue_d),  # lubridate::mdy() to fix date format
    term = factor(term_months),     # turn 'term' into a categorical variable
    delinq_2yrs = factor(delinq_2yrs) # turn 'delinq_2yrs' into a categorical variable
  ) %>% 
  dplyr::select(-emp_title,-installment, -term_months, everything()) #move some not-so-important variables to the end. 


glimpse(lc_clean[, 1:10]) 

skim(lc_clean)
```

The data is now in a clean format stored in the dataframe "lc_clean." 

## **Q1.** Explore the data by building some visualizations as suggested below. Please add at least *two* visualizations of your own.{-}

Provide your answers in the code block below. (Look at the "Lending_Club_Session1_and_2.html" for some hints on how to do this using ggplot.)

```{r, data_visualisation }
# Build a histogram of interest rates. Make sure it looks nice!


# Build a histogram of interest rates but use different color for loans of different grades 
  lc_clean %>%  ggplot( aes(int_rate, fill = grade)) + 
    geom_histogram(bins = 20) + 
    theme_bw() +
    theme(legend.position = "right") +
    labs(title = "Lower risk grades result in lower interest rates charged.",
         subtitle ="Histogram showing the distribution of interest rates segmented per risk grade.",
         caption ="Source: Lending Club (2022)",
         x = "Risk Grade",
         y = "Interest Rate")

# Produce a scatter plot of loan amount against interest rate and add visually the line of best fit
scatterplot_loan_interest <- ggplot(lc_clean, aes(x = loan_amnt, y = int_rate) ) +
  geom_point() +
  geom_smooth() +
  labs(
    title = '10k+ loan amounts is associated with higher interest rate',
    subtitle = 'Relationship between loan amount and interest rate',
    x = 'Loan Amount',
    y = 'Interest Rate',
    caption="Source: Lending Club (2022)"
  ) +
  theme_bw()

scatterplot_loan_interest


# Produce a scatter plot of annual income against interest rate and add visually the line of best fit 


# In the same axes, produce box plots of the interest rate for every value of delinquencies


# Add 2 visualizations of your own
# boxplot of interest rate based on home ownership
lc_clean %>%
  ggplot(aes(x= int_rate, y = home_ownership))+
  geom_boxplot() 

```

# Estimate simple linear regression models

We start with a simple but quite powerful model. Use the `lm` command to estimate a regression model with the following variables "term",  "annual_inc", "dti", and "grade".

```{r, simple regression }

model0 <- lm(int_rate ~ term + annual_inc + dti + grade, data = lc_clean)
summary(model0)

```

# **Q2.** Answer the following questions on model 0. {-}
a. Are all variables statistically significant?

> All variables are significant. 

b. How much explanatory power does the model have? 

>  Around 91.91% of variance in the interest rate is explained by our current model

Fit a new linear regression that is identical to model 0, however, with the extra variable "loan_amnt".

```{r, simple regression with extra variable }

model1 <-lm( int_rate ~ term + annual_inc + dti + grade + loan_amnt, data = lc_clean
  )
summary(model1)

```

# **Q3.** Answer the following questions on model 1. {-}

a. Are all variables statistically significant?

> No, the annual income variable is not significant as the p-value is over 0.05. All others are below 0.05.

b. If your answer to part 'a' is affirmative, then discuss whether model 0 should be preferred over model 1. Otherwise, discuss how a variable that was statistically significant in model 0 is not significant anymore; or how the new variable is not statistically significant.

> Loan amount and annual income are positively correlated, as people with a higher income are more likely to have a higher loan. Adding loan amount, we see that loan amount has a higher explanatory power than annual income. Interest rates are more related to the value of the loan than the income of the person applying. This is logical as well, as a higher income could mean a lower risk and therefore a lower interest, so loan amount is the better variable in this case. 

c. How do you interpret the coefficients of the *(i)* "term60" dummy variable; *(ii)* "gradeF" dummy variable; and *(iii)* "loan_amnt" variable?

> i) Extending the term to 60 months instead of 36 increases the interest rate by 3.608e-03. This is likely due to the increased risk and inflation associated with having a longer debt on your books. 
ii) Clients with a risk profile equivalent to grade F are expected have an interest rate higher than grade A by 1.195e-01 percentage points, which is higher than all grades before. This indicates that these clients have a higher probability of default compared to grade A, which results in a higher interest rate being charged.
iii) For every dollar that the loan amount increases, the interest rate increases by 1.475e-07 percentage point. This means that if the loan amount increases by a million, the interest rate goes up by 0.1475 percentage points. 

d. How much explanatory power does the model have? 

> The model explains 91.97% of the variability interest rates for the Lending Club dataset.

e. Approximately, how wide would the 95% confidence interval of any prediction based on this model be? 

> The confidence interval would be 2 times the residual standard error on each side of the prediction, which is approximately the prediction +/- 0.02112.

# Feature Engineering

Let's build progressively more complex models, with more features.

```{r, Feature Engineering }
#Add to model 1 an interaction between loan amount and grade. Use the "var1*var2" notation to define an interaction term in the linear regression model. This will add the interaction and the individual variables to the model. 

model2 <- lm( int_rate ~ term + annual_inc + dti + grade*loan_amnt, data = lc_clean)
summary(model2)
#Add to the model you just created above the square and the cube of annual income. Use the poly(var_name,3) command as a variable in the linear regression model.  

model3 <-lm( int_rate ~ poly(annual_inc, 3)  + term  + dti + grade*loan_amnt , data = lc_clean)
summary(model3)
#Continuing with the previous model, instead of annual income as a continuous variable break it down into quartiles and use quartile dummy variables. This is an alternative way of modelling non-linear relationships. You can do this with the following command. 
  
lc_clean <- lc_clean %>% 
  mutate(quartiles_annual_inc = as.factor(ntile(annual_inc, 4)))

model4 <- lm( int_rate ~ quartiles_annual_inc  + term  + dti + grade*loan_amnt , data = lc_clean)
summary(model4)  

#Compare the performance of these four models using the `anova` command
anova(model1, model2, model3, model4)
  
```

# **Q4.** Answer the following questions {-}
a. Which of the four models has the most explanatory power in sample? Without testing on a test set, which model would you say has the highest out of sample explanatory power? 

b. In model 1, how do you interpret the estimated coefficient of the interaction term between grade C and loan amount?\
c. The problem of multicollinearity describes the situations where one feature is highly correlated with other features (or with a linear combination of other features). If your goal is to use the model to make predictions, should you be concerned by the problem of multicollinearity? Why, or why not?

>Answer here:


# Out of sample testing
Let's check the predictive accuracy of model2 by holding out a subset of the data to use as a testing data set. This method is sometimes referred to as the hold-out method for out-of-sample testing. The code below is incomplete -- complete in light of the comments.

```{r, out of sample testing }
# split the data in dataframe called "testing" and another one called  "training". The "training" dataframe should have 80% of the data and the "testing" dataframe 20%.
set.seed(23)
train_test_split <- initial_split(lc_clean, prop = 0.80
                                  )
training <- training(train_test_split)
testing <- testing(train_test_split)

#Fit model2 on the training set 
model2_training <- lm( int_rate ~ term + annual_inc + dti + grade*loan_amnt, data = training)
summary(model2_training)

# Calculate the RMSE of the model in the training set (in sample)
rmse_training <- residuals(model2_training)^2 %>% 
              mean() %>% 
              sqrt()

# Use the model to make predictions out of sample in the testing set
pred<-predict(model2_training,testing)
# Calculate the RMSE of the model in the testing set (out of sample)
rmse_testing<- RMSE(pred,testing$int_rate)
rmse_testing
```

# **Q5.** How much does the predictive accuracy of Model 2 deteriorate when we move from in-sample to out-of-sample testing? Is this sensitive to the random seed chosen? Is there any evidence of overfitting? {-}

>Answer here:


# 5-fold cross validation

We can also do out of sample testing using the method of k-fold cross validation which has several advantages over the traditional hold-out method. Using the `caret` package this is easy.

```{r, k-fold cross validation }
#the method "cv" stands for cross validation. We re going to create 10 folds.  

control5 <- trainControl (
    method="cv",
    number=5,
    verboseIter=FALSE) #by setting this to true the model will report its progress after each estimation
control10 <- trainControl (
    method="cv",
    number=10,
    verboseIter=FALSE) #by setting this to true the model will report its progress after each estimation
control15 <- trainControl (
    method="cv",
    number=15,
    verboseIter=FALSE) #by setting this to true the model will report its progress after each estimation

#we are going to train the model and report the results using k-fold cross validation

plsFit<-train(
    int_rate ~ loan_amnt + term+ dti + annual_inc + grade +grade:loan_amnt ,
    lc_clean,
   method = "lm",
    trControl = control5
   )

summary(plsFit)
  
```

# 10-fold cross validation

We can also do out of sample testing using the method of k-fold cross validation which has several advantages over the traditional hold-out method. Using the `caret` package this is easy.

```{r, k-fold cross validation }
#the method "cv" stands for cross validation. We re going to create 10 folds.  

control <- trainControl (
    method="cv",
    number=10,
    verboseIter=TRUE) #by setting this to true the model will report its progress after each estimation

#we are going to train the model and report the results using k-fold cross validation

plsFit<-train(
    int_rate ~ loan_amnt + term+ dti + annual_inc + grade +grade:loan_amnt ,
    lc_clean,
   method = "lm",
    trControl = control
   )

summary(plsFit)
plsFit$results
plsFit$resample$RMSE
mean(plsFit$resample$RMSE)
```

# 15-fold cross validation

We can also do out of sample testing using the method of k-fold cross validation which has several advantages over the traditional hold-out method. Using the `caret` package this is easy.

```{r, k-fold cross validation}
#the method "cv" stands for cross validation. We re going to create 10 folds.  

control <- trainControl (
    method="cv",
    number=15,
    verboseIter=TRUE) #by setting this to true the model will report its progress after each estimation

#we are going to train the model and report the results using k-fold cross validation

plsFit<-train(
    int_rate ~ loan_amnt + term+ dti + annual_inc + grade +grade:loan_amnt ,
    lc_clean,
   method = "lm",
    trControl = control
   )

summary(plsFit)
plsFit$results
plsFit$resample$RMSE
mean(plsFit$resample$RMSE)
  
```



# **Q6.** Compare the out-of-sample RMSE of 10-fold cross validation and the hold-out method. Are they different? Which do you think is more reliable? Are there any drawbacks to the k-fold cross validation method compared to the hold-out method? Determine the out-of-sample RMSE based on 5-fold or 15-fold cross validation.  What you can infer from the outputs about the robusness of your model? {-} 

> 10-fold 0.0105192. Hold-out method 0.01060701. 

# Sample size estimation and learning curves

We can use the hold out method for out-of-sample testing to check if we have a sufficiently large sample to estimate the model reliably. The idea is to set aside some of the data as a testing set. From the remaining data draw progressively larger training sets and check how the performance of the model on the testing set changes. If the performance no longer improves with larger training sets we know we have a large enough sample.  The code below does this. Examine it and run it with different random seeds. 

```{r, learning curves}
# select a testing dataset (25% of all data)
set.seed(12)

train_test_split <- initial_split(lc_clean, prop = 0.75)
remaining <- training(train_test_split)
testing <- testing(train_test_split)

# We are now going to run 30 models starting from a tiny training set drawn from the training data and progressively increasing its size. The testing set remains the same in all iterations.

#initiating the model by setting some parameters to zero
rmse_sample<-0
sample_size<-0
Rsq_sample<-0

for(i in 1:30) {
#from the remaining dataset select a smaller subset to training the data
set.seed(100)
sample

  learning_split <- initial_split(remaining, prop = i/100)
  training <- training(learning_split)
  sample_size[i]=nrow(training)
  
  # training the model on the small dataset
  model3<-lm(int_rate ~ loan_amnt + term+ dti + annual_inc + grade + grade:loan_amnt, training)
  # test the performance of the model on the large testing dataset. This stays fixed for all iterations.
  pred<-predict(model3,testing)
  rmse_sample[i]<-RMSE(pred,testing$int_rate)
  Rsq_sample[i]<-R2(pred,testing$int_rate)
}
plot(sample_size,rmse_sample)
plot(sample_size,Rsq_sample)
```

# **Q7.** Using the learning curves above, approximately how large of a sample size would we need to estimate model 3 reliably? Once we reach this sample size, if we want to reduce the prediction error further what options do we have?{-}

>Answer here:


# Regularization using LASSO regression

If we are in the region of the learning curve where we do not have enough data, one option is to use a regularization method such as LASSO.

Let's try to estimate a large and complicated model (many interactions and polynomials) on a small training dataset using OLS regression and hold-out validation method.

```{r, OLS model overfitting}

#split the data in testing and training. The training test is really small.

set.seed(42)
train_test_split <- initial_split(lc_clean, prop = 0.01)
training <- training(train_test_split)
testing <- testing(train_test_split)

model_lm<-lm(int_rate ~ poly(loan_amnt,3) + term + dti + annual_inc + grade + grade:poly(loan_amnt,3):term + poly(loan_amnt,3):term + grade:term, training)
predictions <- predict(model_lm,testing)

# Model prediction performance

data.frame(
  RMSE = RMSE(predictions, testing$int_rate),
  Rsquare = R2(predictions, testing$int_rate)
)
```

Not surprisingly this model does not perform well -- as we knew form the learning curves we constructed for a simpler model we need a lot more data to estimate this model reliably. Try running it again with different seeds. The model's performance tends to be sensitive to the choice of the training set.

LASSO regression offers one solution -- it extends the OLS regression by penalizing the model for setting any coefficient estimate to a value that is different from zero. The penalty is proportional to a parameter $\lambda$ (pronounced lambda). This parameter cannot be estimated directly (and for this reason sometimes it is referred to as hyperparameter). $\lambda$  will be selected through k-fold cross validation so as to provide the best out-of-sample performance.  As a result of the LASSO procedure, only those features that are more strongly associated with the outcome will have non-zero coefficient estimates and the estimated model will be less sensitive to the training set. Sometimes LASSO regression is referred to as regularization. 
1
```{r, LASSO compared to OLS, warning=FALSE, message=FALSE}
# we will look for the optimal lambda in this sequence (we will try 1000 different lambdas, feel free to try more if necessary)
lambda_seq <- seq(0, 0.01, length = 1000)

# lasso regression using k-fold cross validation to select the best lambda

lasso <- train(
 int_rate ~ poly(loan_amnt,3) + term+ dti + annual_inc + grade +grade:poly(loan_amnt,3):term + poly(loan_amnt,3):term + grade:term,
 data = training,
 method = "glmnet",
  preProc = c("center", "scale"), #This option standardizes the data before running the LASSO regression
  trControl = control,
  tuneGrid = expand.grid(alpha = 1, lambda = lambda_seq) #alpha=1 specifies to run a LASSO regression.
  )

# Model coefficients
coef(lasso$finalModel, lasso$bestTune$lambda)

# Best lambda
lasso$bestTune$lambda

# Count of how many coefficients are greater than zero and how many are equal to zero

sum(coef(lasso$finalModel, lasso$bestTune$lambda)!=0)
sum(coef(lasso$finalModel, lasso$bestTune$lambda)==0)

# Make predictions
predictions <- predict(lasso,testing)

# Model prediction performance

data.frame(
  RMSE = RMSE(predictions, testing$int_rate),
  Rsquare = R2(predictions, testing$int_rate)
)

```

# **Q8.** Answer the following questions {-}
a) Having compared the out-of-sample performance of the OLS regression vs. the LASSO regression, we notice that the latter performs significantly better. This is because the small simple size (1% of the dataset) does not allow us to train OLS regressions properly and results in a poor model. On the other hand, LASSO performs better because it can single out significant coefficients (even on small samples).
b) The value of lambda that offers best performance is 0.0005105105. This value is sensitive to the random seed since it affects the sample set chosen for training, which results in different coefficient estimates, which have to be penalised differently to yield an optimal model, hence the variability in lambda.
c) 24 coefficients are non-zero and 34 are equal to 0 (given seed 876). This number is again sensitive to the random seed chosen since different samples used for training result in different coefficient estimates - from one run to another, a coefficient may turn out to be significant or not. 
d) In using penalised regressions, it is important to standardize all variables to ensure the penalisation is applied equally to all predictors in terms of unit variance.

>Answer here: 

Recall that LASSO regression is almost identical to the OLS, however, there is an extra penalty term that sums the absolute values of the regression coefficients and multiplies this value with some positive penalty parameter. Formally, LASSO uses an $\ell_1$-norm, which is defined as $\ell_1(x) = \sum_{i=1}^n |x_i|$ for $x \in \mathbb{R}^n$. Another popular regularization method is the Ridge regression which penalizes the sum of squares of the coefficients. Formally, the $\ell_2$-norm is used, which is defined as $\ell_2(x) = \sqrt{\sum_{i=1}^n x_i^2}$; however, for the ease of derivations, we typically penalize the sum of squares $\ell_2^2(x) = \sum_{i=1}^n x_i^2$ without loss of generality. Run a Ridge regression and answer the following questions. (Hint. using `alpha=1` in `tuneGrid` called LASSO regression. Find how you can adjust this to call Ridge regression.)

# **Q9.** Answer the following questions {-}
a. Which model performs best out of sample, OLS regression, LASSO, or Ridge? 
b. What value of lambda offers best performance?
c. How many coefficients are zero and how many are non-zero in the Ridge model of best fit? How does it compare with the LASSO regression and what is the reason of the difference?
d. Is it important to standardize continuous variables before running Ridge as in LASSO? Or is it less important for the Ridge regression?
e. In class, we studied the advantages of LASSO regression over the standard OLS. Discuss one potential advantage of using the Ridge regression over the OLS.

>Answer here: 

# Using Time Information

Let's try to further improve the model's predictive performance. So far we have not used any time series information. Effectively, all things being equal, our prediction for the interest rate of a loan given in 2009 would be the same as that of a loan given in 2011. Is this a good assumption?
 
First, investigate graphically whether there are any time trends in the interest rates. (Note that the variable "issue_d" only has information on the month the loan was awarded but not the exact date.) Can you use this information to further improve the forecasting accuracy of your model? Try controlling for time in a linear fashion (i.e., a linear time trend) and controlling for time as quarter-year dummies (this is a method to capture non-linear effects of time -- we assume that the impact of time doesn't change within a quarter but it can chance from quarter to quarter). Finally, check if time affect loans of different grades differently.

```{r, time trends}


#linear time trend -- concentrate on the interest rate as a function of the time (add code below)
 lc_clean %>%
  ggplot( aes(x = issue_d, y = int_rate)) +
  geom_point() +
  geom_smooth(method='lm') +
  labs(
    title = 'Investigation of time trend in interest rates',
    x = 'Time',
    y = 'Interest rate',
    caption = 'Source: Lending Club (2022)'
  ) +
  theme_bw()


#linear time trend by grade -- similar plot, however, group by grades (via different colors) (add code below)
 lc_clean %>%
  ggplot( aes(x = issue_d, y = int_rate, color = grade)) +
  geom_point() +
  geom_smooth(method='lm') +
  labs(
    title = 'Timetrend for interest rate, by rating',
    x = 'Time',
    y = 'Interest rate',
    caption = 'Source: Lending Club (2022)'
  ) +
  theme_bw()

time0<-train(
  int_rate ~ term + annual_inc + dti + grade*loan_amnt,#fill your variables here "+ issue_d"
  lc_clean,
  method = "lm",
  trControl = trainControl(method = 'cv', number = 10)
)
summary(time0)

#Train models using OLS regression and k-fold cross-validation
#The first model has some explanatory variables and a linear time trend

time1<-train(
  int_rate ~ term + annual_inc + dti + grade*loan_amnt + issue_d,#fill your variables here "+ issue_d"
  lc_clean,
  method = "lm",
  trControl = trainControl(method = 'cv', number = 10)
)
summary(time1)

#The second model has a different linear time trend for each grade class (hint. use interactions)
time2<-train(
    int_rate ~ term + annual_inc + dti + grade*loan_amnt + grade:issue_d, #fill your variables here 
    lc_clean,
   method = "lm",
    trControl = trainControl(method = 'cv', number = 10)
   )
summary(time2)

#Change the time trend to a quarter dummy variables.
#zoo::as.yearqrt() creates quarter dummies 
lc_clean_quarter<-lc_clean %>%
  mutate(yq = as.factor(as.yearqtr(lc_clean$issue_d, format = "%Y-%m-%d")))

time3<-train(
    int_rate ~ term + annual_inc + dti + grade*loan_amnt + yq,#fill your variables here 
    lc_clean_quarter,
     method = "lm",
    trControl = trainControl(method = 'cv', number = 10)
   )
summary(time3)

#We specify one quarter dummy variable for each grade. This is going to be a large model as there are 19 quarters x 7 grades = 133 quarter-grade dummies.
time4<-train(
    int_rate ~  term + annual_inc + dti + grade*loan_amnt + grade:yq,#fill your variables here 
    lc_clean_quarter,
     method = "lm",
    trControl = trainControl(method = 'cv', number = 10)
   )
summary(time4)

data.frame(
  time0$results$RMSE,
  time1$results$RMSE,
  time2$results$RMSE,
  time3$results$RMSE,
  time4$results$RMSE)
```

# **Q10.** Based on your analysis above, is there any evidence to suggest that interest rates change over time? Does including time trends / quarter-year dummies improve predictions? Any improvement in prediction performance? {-}

> According to the graphical exploratory analysis, the relationship between interest rate and time is not very pronounced. When taking grade into account however, the relationship is more pronounced for some grades and less existent for other grades. For grade G, the relationship is even negative while the others are positive. This indicates that the investors are becoming more risk-averse, as the interst rate for high-risk loans is getting higher as time progresses as their risk of default is higher. The interest rate of the lowest risk grade is decreasing, as this is a more attractive option for the lenders as the risk of default is lower, since these borrowers have a better credit record. By lowering the interest rate, we are lowering the cost of borrowing money, incentivizing the low-risk population to borrow more, increasing our opportunity for lending. 
This would indicate that we need to take an interaction between grade and timetrend into account. We can see this in the RSME as well as the models with an interaction with grade have a lower RMSE than the same model without an interaction.
Comparing to a model without a time variable, we see that even the only linear model (time1) decreases the RMSE by 1.916%. The best model, using quarterly dummies and a grade interaction, decreased the RMSE by 27.94% compared to the model without a time variable. The length of the confidence interval of any prediction for the best model (time4) is 2* 1.96 * RMSE = 0.02971, which is even smaller than the 0.04123 confidence interval length of the model that does not take time into account.


# Using Bond Yields 
One concern with using time trends for forecasting is that in order to make predictions for future loans we will need to project trends to the future. This is an extrapolation that may not be reasonable, especially if macroeconomic conditions in the future change. Furthermore, if we are using quarter-year dummies, it is not even possible to estimate the coefficient of these dummy variables for future quarters.

Instead, perhaps it is better to find the reasons as to why different periods are different from one another. The csv file "MonthBondYields.csv" contains information on the yield of US Treasuries on the first day of each month. Can you use it to see if you can improve your predictions without using time dummies? 


```{r, bond yields, eval = F}
#load the data to memory as a dataframe
bond_prices<-readr::read_csv("MonthBondYields .csv")

#make the date of the bond file comparable to the lending club dataset
#for some regional date/number (locale) settings this may not work. If it does try running the following line of code in the Console
#Sys.setlocale("LC_TIME","English")
bond_prices <- bond_prices %>%
  mutate(Date2=as.Date(paste("01",Date,sep="-"),"%d-%b-%y")) %>%
  select(-starts_with("X"))

#let's see what happened to bond yields over time. Lower bond yields mean the cost of borrowing has gone down.

bond_prices %>%
  ggplot(aes(x=Date2, y=Price))+geom_point(size=0.1, alpha=0.5)

#join the data using a left join
lc_with_bonds<-lc_clean %>%
  left_join(bond_prices, by = c("issue_d" = "Date2")) %>%
  arrange(issue_d) %>%
  filter(!is.na(Price)) #drop any observations where there re no bond prices available

# investigate graphically if there is a relationship 
lc_with_bonds%>%
  ggplot(aes(x=int_rate, y=Price))+geom_point(size=0.1, alpha=0.5)+geom_smooth(method="lm")

lc_with_bonds%>%
  ggplot(aes(x=int_rate, y=Price, color=grade))+geom_point(size=0.1, alpha=0.5)+geom_smooth(method="lm")

#let's train a model using the bond information

plsFit<-train(
    int_rate ~  , #fill your variables here -- try adding Price and grade.
    lc_with_bonds,
   method = "lm",
    trControl = #add 10-fold CV control here
   )
summary(plsFit)
```
# **Q11.** Do bond yields have any explanatory power? Do not forget to interpret coefficients. {-}

>Answer here: 

# **Q12.** Choose a model and describe your methodology {-}
Feel free to investigate more models with different features using the methodologies covered so far. Present the model you believe predicts interest rates the best. Describe how good it is (including the length of the 95% Confidence Interval of predictions that use this model) and what features it uses. What methodology did you use to choose it? (Do not use time trends or quarter-year dummies in your model as the first cannot be extrapolated into the future reliably and the second cannot be even estimated for future quarters.)

> We followed an EDA (Explore, Diagnose, Analyse) process to understand the lc_clean dataset, and extracted variables that we believe could provide insight on our response variable.
We analysed the following variables to understand if they could provide insight on the interest rate per loans in addition to the variables that we have already selected: verification_status, instalment, employment length
We mutated the data to create categories within the dataset, and then performed t-tests to understand if there were true differences in the mean.
We concluded that instalment categories and verification status categories had significant differences in means, and we have decided to include it in the model.
Employment length did not have an impact on the interest rate.
We have included a variable for verification, with 2 levels - verified or not verified. Based on a t-test, the level of interest rates for these two levels is significantly different. 

```{r}

glimpse(lc_clean)

```


```{r}
list(
  emp_length = unique(lc_clean$emp_length),
  home_ownership = unique(lc_clean$home_ownership),
  verification_status = unique(lc_clean$verification_status),
  loan_status = unique(lc_clean$loan_status),
  purpose = unique(lc_clean$purpose)
)

lc_clean %>%
  ggplot(aes(x=verification_status, y = int_rate)) +
  geom_boxplot() +
  theme_bw()

lc_clean_2 <- lc_clean %>%
  mutate(verification_2levels = ifelse(verification_status == 'Not Verified', 0, 1),
         installment_2stages = as.factor(ntile(installment, 2)),
         grades_comb = case_when (
           grade == 'F' ~ 'F+',
           grade == 'G' ~ 'F+',
           TRUE ~ grade),
         quartiles_annual_inc = as.factor(ntile(annual_inc, 4))
         )

#unique(lc_clean_2$grades_comb)

t.test(int_rate ~ verification_2levels, lc_clean_2)
t.test(int_rate ~ installment_2stages, lc_clean_2)

```






```{r}
lc_clean %>%
  select(int_rate, installment, purpose, verification_status, home_ownership) %>%
  ggpairs()

```

```{r}
lc_clean %>%
  mutate(installment_2stages = as.factor(ntile(installment, 2))) %>%
  ggplot(aes(x = installment_2stages, y = int_rate)) +
  geom_boxplot() 
```

```{r}
lc_clean %>%
  ggplot(aes(x=emp_length, y=int_rate)) +
  geom_boxplot()
         
    
```
```{r}
set.seed(42)
train_test_split <- initial_split(lc_clean_2, prop = 0.01)
training <- training(train_test_split)
testing <- testing(train_test_split)
```

```{r}
glimpse(lc_clean_2)

q12_OLS <- train(
 int_rate ~ quartiles_annual_inc  + term  + dti + grades_comb*loan_amnt + verification_2levels + installment_2stages,
 data = lc_clean_2,
 method = 'lm',
  trControl = control10,
  )

q12_lasso <- train(
 int_rate ~ quartiles_annual_inc  + term  + dti + grades_comb*loan_amnt + verification_2levels + installment_2stages + grades_comb,
 data = lc_clean_2,
 method = "glmnet",
  preProc = c("center", "scale"), #This option standardizes the data before running the LASSO regression
  trControl = control10,
  tuneGrid = expand.grid(alpha = 1, lambda = lambda_seq) #alpha=1 specifies to run a LASSO regression.
  )

q12_ridge <- train(
 int_rate ~ quartiles_annual_inc  + term  + dti + grades_comb*loan_amnt + verification_2levels + installment_2stages,
 data = lc_clean_2,
 method = "glmnet",
  preProc = c("center", "scale"), #This option standardizes the data before running the LASSO regression
  trControl = control10,
  tuneGrid = expand.grid(alpha = 0, lambda = lambda_seq) #alpha=1 specifies to run a LASSO regression.
  )

```

```{r}
# Make predictions
predictions <- predict(q12_lasso,testing)

# Model prediction performance

data.frame(
  RMSE = RMSE(predictions, testing$int_rate),
  Rsquare = R2(predictions, testing$int_rate)
)

predictions <- predict(q12_ridge,testing)

# Model prediction performance

data.frame(
  RMSE = RMSE(predictions, testing$int_rate),
  Rsquare = R2(predictions, testing$int_rate)
)


q12_OLS$results$RMSE

summary(q12_OLS)

```


>Answer here:

# **Q13.** (optional) Use other publicly available datasets to further improve performance (e.g., quarterly data on US inflation, [CPI](https://fred.stlouisfed.org/series/CPALTT01USQ657N), [unemployment rate](https://fred.stlouisfed.org/series/UNRATE)). Explain why you think the additional data will make a difference and check if it does. {-}

